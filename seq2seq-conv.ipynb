{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-14T11:02:55.488145Z","iopub.execute_input":"2023-05-14T11:02:55.488562Z","iopub.status.idle":"2023-05-14T11:02:55.495519Z","shell.execute_reply.started":"2023-05-14T11:02:55.488530Z","shell.execute_reply":"2023-05-14T11:02:55.494490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122).\n\n![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/convseq2seq0.png?raw=1)","metadata":{}},{"cell_type":"markdown","source":"Cвёрточный слой использует *фильтры*. Эти фильтры имеют *ширину* (а также *высоту* в изображениях, но обычно не для текстов). Если фильтр имеет ширину 3, он может видеть 3 последовательных токена. Каждый свёрточный слой имеет множество таких фильтров (1024 в этом разделе). Каждый фильтр будет скользить по последовательности от начала до конца, просматривая все 3 последовательных токена одновременно. Идея состоит в том, что каждый из этих 1024 фильтров научится извлекать из текста разные признаки. Результат этого извлечения признаков будет затем использоваться моделью — потенциально в качестве входных данных для другого свёрточного слоя. Затем всё это можно использовать для извлечения особенностей из исходного предложения для перевода его на целевой язык.","metadata":{}},{"cell_type":"code","source":"path_do_data = '../../datasets/Machine_translation_EN_RU/data.txt'\nif not os.path.exists(path_do_data):\n    print(\"Dataset not found locally. Downloading from github.\")\n    !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/datasets/Machine_translation_EN_RU/data.txt -nc\n    path_do_data = './data.txt'","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:02:55.497503Z","iopub.execute_input":"2023-05-14T11:02:55.498518Z","iopub.status.idle":"2023-05-14T11:02:56.558869Z","shell.execute_reply.started":"2023-05-14T11:02:55.498484Z","shell.execute_reply":"2023-05-14T11:02:56.557663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install subword_nmt\n!pip install torchtext==0.6.0","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:02:56.561514Z","iopub.execute_input":"2023-05-14T11:02:56.562277Z","iopub.status.idle":"2023-05-14T11:03:18.605384Z","shell.execute_reply.started":"2023-05-14T11:02:56.562235Z","shell.execute_reply":"2023-05-14T11:03:18.604183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# old deprecated code\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torchtext\nfrom torchtext.datasets import TranslationDataset, Multi30k\nfrom torchtext.data import Field, BucketIterator, TabularDataset\n\nimport spacy\nimport tqdm\nimport random\nimport math\nimport time\n\nimport matplotlib\nmatplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import clear_output\n\nfrom nltk.tokenize import WordPunctTokenizer\nfrom subword_nmt.learn_bpe import learn_bpe\nfrom subword_nmt.apply_bpe import BPE","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:03:18.608433Z","iopub.execute_input":"2023-05-14T11:03:18.608830Z","iopub.status.idle":"2023-05-14T11:03:18.619902Z","shell.execute_reply.started":"2023-05-14T11:03:18.608792Z","shell.execute_reply":"2023-05-14T11:03:18.618934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_W = WordPunctTokenizer()\ndef tokenize(x, tokenizer=tokenizer_W):\n    return tokenizer.tokenize(x.lower())","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:03:18.623037Z","iopub.execute_input":"2023-05-14T11:03:18.624041Z","iopub.status.idle":"2023-05-14T11:03:18.635795Z","shell.execute_reply.started":"2023-05-14T11:03:18.624007Z","shell.execute_reply":"2023-05-14T11:03:18.634797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U spacy==3.0\n!python -m spacy download en_core_web_sm\n!python -m spacy download ru_core_news_sm","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:03:18.637267Z","iopub.execute_input":"2023-05-14T11:03:18.637703Z","iopub.status.idle":"2023-05-14T11:08:50.415629Z","shell.execute_reply.started":"2023-05-14T11:03:18.637672Z","shell.execute_reply":"2023-05-14T11:08:50.414393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spacy_de = spacy.load('ru_core_news_sm')\nspacy_en = spacy.load('en_core_web_sm')\ndef tokenize_ru(text):\n    \"\"\"\n    Tokenizes Russian text from a string into a list of strings\n    \"\"\"\n    return [tok.text for tok in spacy_de.tokenizer(text)]\n\ndef tokenize_en(text):\n    \"\"\"\n    Tokenizes English text from a string into a list of strings\n    \"\"\"\n    return [tok.text for tok in spacy_en.tokenizer(text)]","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:08:50.417690Z","iopub.execute_input":"2023-05-14T11:08:50.418040Z","iopub.status.idle":"2023-05-14T11:08:53.969304Z","shell.execute_reply.started":"2023-05-14T11:08:50.418010Z","shell.execute_reply":"2023-05-14T11:08:53.968312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Кодировщик\n\nСвёрточная модель sequence-to-sequence получает два вектора контекста для каждого токена во входном предложении. Итак, если бы в нашем входном предложении было 6 токенов, мы получили бы 12 контекстных векторов, по два на каждый токен.\n\nДва вектора контекста на токен - это *преобразованный* вектор и *комбинированный* вектор. Преобразованный вектор - это результат прохождения каждого токена через несколько слоев. Комбинированный вектор получается в результате суммирования вектора, прошедшего свёрточные слои, и эмбеддинга этого токена. Оба они возвращаются кодером для использования декодером.","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass ConvolutionEncoder(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 emb_dim, \n                 hid_dim, \n                 n_layers, \n                 kernel_size, \n                 dropout, \n                 device,\n                 max_length = 256):\n        super().__init__()\n        \n        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n        \n        self.device = device\n        \n        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n                                              out_channels = 2 * hid_dim, \n                                              kernel_size = kernel_size, \n                                              padding = (kernel_size - 1) // 2)\n                                    for _ in range(n_layers)])\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src):\n        #src = [batch size, src len]\n        batch_size = src.shape[0]\n        src_len = src.shape[1]\n        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n        #pos = [0, 1, 2, 3, ..., src len - 1]\n        #pos = [batch size, src len]\n        tok_embedded = self.tok_embedding(src)\n        pos_embedded = self.pos_embedding(pos)\n        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\n        embedded = self.dropout(tok_embedded + pos_embedded)\n        #embedded = [batch size, src len, emb dim]\n        conv_input = self.emb2hid(embedded)\n        #conv_input = [batch size, src len, hid dim]\n        conv_input = conv_input.permute(0, 2, 1) \n        #conv_input = [batch size, hid dim, src len]\n        for i, conv in enumerate(self.convs):\n            conved = conv(self.dropout(conv_input))\n            #conved = [batch size, 2 * hid dim, src len]\n            #pass through GLU activation function\n            conved = F.glu(conved, dim = 1)\n            #conved = [batch size, hid dim, src len]\n            #apply residual connection\n            conved = (conved + conv_input) * self.scale\n            #conved = [batch size, hid dim, src len]\n            #set conv_input to conved for next loop iteration\n            conv_input = conved\n        conved = self.hid2emb(conved.permute(0, 2, 1))\n        #conved = [batch size, src len, emb dim]\n        combined = (conved + embedded) * self.scale\n        #combined = [batch size, src len, emb dim]\n        return conved, combined","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:08:53.970721Z","iopub.execute_input":"2023-05-14T11:08:53.971329Z","iopub.status.idle":"2023-05-14T11:08:53.984533Z","shell.execute_reply.started":"2023-05-14T11:08:53.971293Z","shell.execute_reply":"2023-05-14T11:08:53.983476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Декодер принимает фактическое целевое предложение и пытается его предсказать. Эта модель прогнозирует все токены в целевом предложении параллельно. Нет последовательной обработки, то есть цикла декодирования. \n\nДекодер похож на кодировщик, с некоторыми изменениями как в основной модели, так и в свёрточных блоках внутри модели.\n\n![](https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/assets/convseq2seq3.png?raw=1)","metadata":{}},{"cell_type":"code","source":"class ConvolutionDecoder(nn.Module):\n    def __init__(self, \n                 output_dim, \n                 emb_dim, \n                 hid_dim, \n                 n_layers, \n                 kernel_size, \n                 dropout, \n                 trg_pad_idx, \n                 device,\n                 max_length = 256):\n        super().__init__()\n        \n        self.kernel_size = kernel_size\n        self.trg_pad_idx = trg_pad_idx\n        self.device = device\n        \n        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n        \n        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n        \n        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n        \n        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n        \n        self.fc_out = nn.Linear(emb_dim, output_dim)\n        \n        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n                                              out_channels = 2 * hid_dim, \n                                              kernel_size = kernel_size)\n                                    for _ in range(n_layers)])\n        \n        self.dropout = nn.Dropout(dropout)\n      \n    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n        #embedded = [batch size, trg len, emb dim]\n        #conved = [batch size, hid dim, trg len]\n        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n        #conved_emb = [batch size, trg len, emb dim]\n        combined = (conved_emb + embedded) * self.scale\n        #combined = [batch size, trg len, emb dim]\n        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n        #energy = [batch size, trg len, src len]\n        attention = F.softmax(energy, dim=2)\n        #attention = [batch size, trg len, src len]\n        attended_encoding = torch.matmul(attention, encoder_combined)\n        #attended_encoding = [batch size, trg len, emd dim]\n        attended_encoding = self.attn_emb2hid(attended_encoding)\n        #attended_encoding = [batch size, trg len, hid dim]\n        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n        #attended_combined = [batch size, hid dim, trg len]\n        return attention, attended_combined\n        \n    def forward(self, trg, encoder_conved, encoder_combined):\n        #trg = [batch size, trg len]\n        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n        batch_size = trg.shape[0]\n        trg_len = trg.shape[1]\n        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n        #pos = [batch size, trg len]\n        tok_embedded = self.tok_embedding(trg)\n        pos_embedded = self.pos_embedding(pos)\n        #tok_embedded = [batch size, trg len, emb dim]\n        #pos_embedded = [batch size, trg len, emb dim]\n        embedded = self.dropout(tok_embedded + pos_embedded)\n        conv_input = self.emb2hid(embedded)\n        #conv_input = [batch size, trg len, hid dim]\n        conv_input = conv_input.permute(0, 2, 1) \n        #conv_input = [batch size, hid dim, trg len]\n        batch_size = conv_input.shape[0]\n        hid_dim = conv_input.shape[1]\n        \n        for i, conv in enumerate(self.convs):\n            conv_input = self.dropout(conv_input)\n            padding = torch.zeros(batch_size, \n                                  hid_dim, \n                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\n            conved = conv(padded_conv_input)\n            #conved = [batch size, 2 * hid dim, trg len]\n            conved = F.glu(conved, dim = 1)\n            #conved = [batch size, hid dim, trg len]\n            attention, conved = self.calculate_attention(embedded, \n                                                         conved, \n                                                         encoder_conved, \n                                                         encoder_combined)\n            #attention = [batch size, trg len, src len]\n            conved = (conved + conv_input) * self.scale\n            #conved = [batch size, hid dim, trg len]\n            conv_input = conved\n            \n        conved = self.hid2emb(conved.permute(0, 2, 1))\n        #conved = [batch size, trg len, emb dim] \n        output = self.fc_out(self.dropout(conved))\n        #output = [batch size, trg len, output dim]\n        return output, attention","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:08:53.986827Z","iopub.execute_input":"2023-05-14T11:08:53.987416Z","iopub.status.idle":"2023-05-14T11:08:54.004615Z","shell.execute_reply.started":"2023-05-14T11:08:53.987384Z","shell.execute_reply":"2023-05-14T11:08:54.003679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Кодирование - вставляем исходную последовательность и получаем «вектор контекста». Однако здесь два вектора контекста на одно слово в исходной последовательности, `encoder_conved` и `encoder_combined`.\n\nПоскольку декодирование выполняется параллельно, нам не нужен цикл декодирования. Вся целевая последовательность сразу вводится в декодер, а заполнение используется для обеспечения того, чтобы каждый свёрточный фильтр в декодере мог видеть только текущий и предыдущий токены в последовательности, когда он скользит по предложению.","metadata":{}},{"cell_type":"code","source":"class ConvolutionSeq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, src, trg):\n        #src = [batch size, src len]\n        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n        encoder_conved, encoder_combined = self.encoder(src)\n        #encoder_conved = [batch size, src len, emb dim]\n        #encoder_combined = [batch size, src len, emb dim]\n        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n        #output = [batch size, trg len - 1, output dim]\n        #attention = [batch size, trg len - 1, src len]\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:08:54.006183Z","iopub.execute_input":"2023-05-14T11:08:54.006531Z","iopub.status.idle":"2023-05-14T11:08:54.016502Z","shell.execute_reply.started":"2023-05-14T11:08:54.006500Z","shell.execute_reply":"2023-05-14T11:08:54.015525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flatten(l):\n    return [item for sublist in l for item in sublist]\n\ndef remove_tech_tokens(mystr, tokens_to_remove=['<eos>', '<sos>', '<unk>', '<pad>']):\n    return [x for x in mystr if x not in tokens_to_remove]\n\n\ndef get_text(x, TRG_vocab):\n    text = [TRG_vocab.itos[token] for token in x]\n    try:\n        end_idx = text.index('<eos>')\n        text = text[:end_idx]\n    except ValueError:\n        pass\n    text = remove_tech_tokens(text)\n    if len(text) < 1:\n        text = []\n    return text\n\n\ndef generate_translation(src, trg, model, TRG_vocab):\n    model.eval()\n\n    output = model(src, trg, 0) #turn off teacher forcing\n    output = output.argmax(dim=-1).cpu().numpy()\n\n    original = get_text(list(trg[:,0].cpu().numpy()), TRG_vocab)\n    generated = get_text(list(output[1:, 0]), TRG_vocab)\n    \n    print('Original: {}'.format(' '.join(original)))\n    print('Generated: {}'.format(' '.join(generated)))\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:08:54.020509Z","iopub.execute_input":"2023-05-14T11:08:54.020798Z","iopub.status.idle":"2023-05-14T11:08:54.030361Z","shell.execute_reply.started":"2023-05-14T11:08:54.020775Z","shell.execute_reply":"2023-05-14T11:08:54.029429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:08:54.031826Z","iopub.execute_input":"2023-05-14T11:08:54.032474Z","iopub.status.idle":"2023-05-14T11:08:54.041971Z","shell.execute_reply.started":"2023-05-14T11:08:54.032443Z","shell.execute_reply":"2023-05-14T11:08:54.041026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SRC = Field(tokenize=tokenize_ru,\n            init_token = '<sos>', \n            eos_token = '<eos>', \n            lower = True,\n            batch_first=True)\n\nTRG = Field(tokenize=tokenize_en,\n            init_token = '<sos>', \n            eos_token = '<eos>', \n            lower = True,            \n            batch_first=True)\n\n\ndataset = TabularDataset(\n    path=path_do_data,\n    format='tsv',\n    fields=[('trg', TRG), ('src', SRC)]\n)\n\ntrain_data, valid_data, test_data = dataset.split(split_ratio=[0.8, 0.15, 0.05])\n\n\nSRC.build_vocab(train_data, min_freq = 3)\nTRG.build_vocab(train_data, min_freq = 3)\n\ndef _len_sort_key(x):\n    return len(x.src)\n\nBATCH_SIZE = 128\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size = BATCH_SIZE, \n    device = device,\n    sort_key=_len_sort_key\n)\n\nINPUT_DIM = len(SRC.vocab)\nOUTPUT_DIM = len(TRG.vocab)\nEMB_DIM = 64\nHID_DIM = 128 # each conv. layer has 2 * hid_dim filters\nENC_LAYERS = 10 # number of conv. blocks in encoder\nDEC_LAYERS = 10 # number of conv. blocks in decoder\nENC_KERNEL_SIZE = 3 # must be odd!\nDEC_KERNEL_SIZE = 3 # can be even or odd\nENC_DROPOUT = 0.25\nDEC_DROPOUT = 0.25\nTRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n\nenc = ConvolutionEncoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\ndec = ConvolutionDecoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\n\nmodel_two = ConvolutionSeq2Seq(enc, dec).to(device)\noptimizer = optim.Adam(model_two.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:08:54.043362Z","iopub.execute_input":"2023-05-14T11:08:54.043664Z","iopub.status.idle":"2023-05-14T11:09:09.097598Z","shell.execute_reply.started":"2023-05-14T11:08:54.043641Z","shell.execute_reply":"2023-05-14T11:09:09.096607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Для всех моделей мы никогда не помещаем `<eos>` в декодер. В моделях RNN это объяснялось тем, что цикл декодера останавливается при достихении значения `<eos>` в качестве входа для декодера. В этой модели мы просто отрезаем токен `<eos>` от конца последовательности. Таким образом:\n\n$$\\begin{align*}\n\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n\\end{align*}$$\n\n$x_i$ обозначает фактический элемент целевой последовательности. Затем вводим это в модель, чтобы получить предсказанную последовательность, которая должна предсказывать токен `<eos>`:\n\n$$\\begin{align*}\n\\text{output} &= [y_1, y_2, y_3, eos]\n\\end{align*}$$\n\n$y_i$ обозначает предсказанный элемент целевой последовательности. Затем вычисляем наши потери, используя исходный тензор `trg` с токеном `<sos> `, отрезанным спереди, оставляя токен `<eos> `:\n\n$$\\begin{align*}\n\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n\\end{align*}$$\n\nЗатем рассчитываем потери и обновляем параметры.","metadata":{}},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None):\n    model.train()\n    \n    epoch_loss = 0\n    history = []\n    for i, batch in enumerate(iterator):\n        \n        src = batch.src\n        trg = batch.trg\n        \n        optimizer.zero_grad()\n        \n        output = model(src, trg)\n        \n        #trg = [trg sent len, batch size]\n        #output = [trg sent len, batch size, output dim]\n        \n        output = output[1:].view(-1, output.shape[-1])\n        trg = trg[1:].view(-1)\n        \n        #trg = [(trg sent len - 1) * batch size]\n        #output = [(trg sent len - 1) * batch size, output dim]\n        \n        loss = criterion(output, trg)\n        \n        loss.backward()\n        \n        # Let's clip the gradient\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n        history.append(loss.cpu().data.numpy())\n        if (i+1)%10==0:\n            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n\n            clear_output(True)\n            ax[0].plot(history, label='train loss')\n            ax[0].set_xlabel('Batch')\n            ax[0].set_title('Train loss')\n            if train_history is not None:\n                ax[1].plot(train_history, label='general train history')\n                ax[1].set_xlabel('Epoch')\n            if valid_history is not None:\n                ax[1].plot(valid_history, label='general valid history')\n            plt.legend()\n            \n            plt.show()\n\n        \n    return epoch_loss / len(iterator)\n\n# Цикл оценки такой же, как цикл обучения, только без вычислений градиента и обновления параметров.\ndef evaluate_model_two(model, iterator, criterion):\n    \n    model.eval()\n    epoch_loss = 0\n    history = []\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            src = batch.src\n            trg = batch.trg\n\n            output = model(src, trg)\n            #output = [batch size, trg len - 1, output dim]\n            #trg = [batch size, trg len]\n            output = output[1:].view(-1, output.shape[-1])\n            trg = trg[1:].view(-1)\n            #output = [batch size * trg len - 1, output dim]\n            #trg = [batch size * trg len - 1]\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:09:09.099094Z","iopub.execute_input":"2023-05-14T11:09:09.099471Z","iopub.status.idle":"2023-05-14T11:09:09.113857Z","shell.execute_reply.started":"2023-05-14T11:09:09.099437Z","shell.execute_reply":"2023-05-14T11:09:09.112753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# сколько времени занимает каждая эпоха\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:16:21.583998Z","iopub.execute_input":"2023-05-14T11:16:21.584725Z","iopub.status.idle":"2023-05-14T11:16:21.590156Z","shell.execute_reply.started":"2023-05-14T11:16:21.584686Z","shell.execute_reply":"2023-05-14T11:16:21.589145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_history = []\nvalid_history = []\n\nN_EPOCHS = 10\nCLIP = 1\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model_two, train_iterator, optimizer, criterion, CLIP, train_history, valid_history)\n    valid_loss = evaluate_model_two(model_two, valid_iterator, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model_two.state_dict(), 'conv-model.pt')\n    \n    train_history.append(train_loss)\n    valid_history.append(valid_loss)\n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:16:23.107166Z","iopub.execute_input":"2023-05-14T11:16:23.107544Z","iopub.status.idle":"2023-05-14T11:21:55.743132Z","shell.execute_reply.started":"2023-05-14T11:16:23.107515Z","shell.execute_reply":"2023-05-14T11:21:55.742080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_text = []\ngenerated_text = []\nmodel_two.eval()\nwith torch.no_grad():\n\n    for i, batch in tqdm.tqdm(enumerate(test_iterator)):\n\n        src = batch.src\n        trg = batch.trg\n\n        output = model_two(src, trg) #turn off teacher forcing\n\n        #trg = [trg sent len, batch size]\n        #output = [trg sent len, batch size, output dim]\n\n        output = output.argmax(dim=-1)\n        \n        original_text.extend([get_text(x, TRG.vocab) for x in trg.cpu().numpy().T])\n        generated_text.extend([get_text(x, TRG.vocab) for x in output[1:].detach().cpu().numpy().T])","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:21:55.745301Z","iopub.execute_input":"2023-05-14T11:21:55.745680Z","iopub.status.idle":"2023-05-14T11:21:56.702189Z","shell.execute_reply.started":"2023-05-14T11:21:55.745644Z","shell.execute_reply":"2023-05-14T11:21:56.701243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\ncorpus_bleu([[text] for text in original_text], generated_text) * 100\n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T11:28:33.392690Z","iopub.execute_input":"2023-05-14T11:28:33.393234Z","iopub.status.idle":"2023-05-14T11:28:34.295761Z","shell.execute_reply.started":"2023-05-14T11:28:33.393193Z","shell.execute_reply":"2023-05-14T11:28:34.294688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}